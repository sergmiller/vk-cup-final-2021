{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(BASE_PATH + 'rus_train_dataset.csv', encoding='utf-8', sep='|')\n",
    "en_ru_aux_data = pd.read_csv(BASE_PATH + 'en_to_rus_train_dataset.csv', encoding='utf-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_targets = train_data[\"right_answer_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 538 ms, sys: 899 ms, total: 1.44 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_bundle = np.load(BASE_PATH + 'train_dataset_bundle.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 362 ms, sys: 676 ms, total: 1.04 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ext_bundle = np.load(BASE_PATH + 'en_ru_aux_dataset_bundle.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fix_input(x):\n",
    "    if x in ['0.0', '1.0', '2.0', '0,0', '1,0', '2,0', '1.0.']:\n",
    "        return float(x[0])\n",
    "    return -1\n",
    "\n",
    "prepared_en_ru_targets = np.array([_fix_input(x) for x in en_ru_aux_data[\"right_answer_id\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, question, answer, y):\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.BCEWithLogitsLoss()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(train, val, model_ff, epochs=5, batch_size=64, shuffle=True, freq=10,lr=1e-3,criterion = nn.BCEWithLogitsLoss()): \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    ids_nn = np.arange(train.y.shape[0])\n",
    "    \n",
    "#     reshape_to_last = lambda x: torch.reshape(x, [np.prod(x.shape[:-1]), x.shape[-1]])\n",
    "\n",
    "    optimizer = optim.Adam(model_ff.parameters(), lr=lr)\n",
    "\n",
    "    time_for_print_loss = lambda i: (i + 1) % freq == 0\n",
    "    \n",
    "    n_iter = 0\n",
    "    \n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "        np.random.shuffle(ids_nn)\n",
    "\n",
    "        model_ff.train(True)\n",
    "\n",
    "        for b in np.arange(0, train.y.shape[0], batch_size):\n",
    "            X_batch = torch.FloatTensor(train.question[ids_nn[b:b+batch_size]])\n",
    "            y_batch = torch.FloatTensor(train.y[ids_nn[b:b+batch_size]])\n",
    "            a_batch = torch.FloatTensor(train.answer[ids_nn[b:b+batch_size]])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred_logits = model_ff(X_batch, a_batch)\n",
    "\n",
    "            loss = criterion(y_pred_logits, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (b // batch_size + 1) % freq == 0:\n",
    "                print('train loss in %d epoch in %d batch: %.5f' %\n",
    "                  (epoch + 1, b // batch_size + 1, loss.item()))\n",
    "                \n",
    "                writer.add_scalar('data/train_loss', loss.item(), n_iter)\n",
    "                writer.add_scalar('data/epoch', epoch + 1, n_iter)\n",
    "                writer.add_scalar('data/batch', b // batch_size + 1, n_iter)\n",
    "\n",
    "                val_loss = 0\n",
    "                val_acc = 0\n",
    "                its = 0\n",
    "                model_ff.train(False)\n",
    "                for b in np.arange(0, val.y.shape[0], batch_size):\n",
    "                    its += 1\n",
    "                    X_batch = torch.FloatTensor(val.question[b:b+batch_size])\n",
    "                    y_batch = torch.FloatTensor(val.y[b:b+batch_size])\n",
    "                    a_batch = torch.FloatTensor(val.answer[b:b+batch_size])\n",
    "                    with torch.no_grad():\n",
    "                        y_pred_logits = model_ff(X_batch, a_batch)\n",
    "                    loss = criterion(y_pred_logits, y_batch)\n",
    "#                     print(X_batch.shape, y_batch.shape, y_pred_logits.shape)\n",
    "                    s_pred = torch.argmax(y_pred_logits.reshape(-1, 3), dim=1)\n",
    "                    s_true = torch.argmax(y_batch.reshape(-1, 3), dim=1)\n",
    "                    acc = torch.mean((s_pred == s_true).type(torch.FloatTensor))\n",
    "                    val_loss += loss.item()\n",
    "                    val_acc += acc.item()\n",
    "                val_loss /= its\n",
    "                val_acc /= its\n",
    "                print('val loss in %d epoch: %.5f' % (epoch + 1, val_loss))\n",
    "                print('val acc in %d epoch: %.5f' % (epoch + 1, val_acc))\n",
    "                \n",
    "                writer.add_scalar('data/val_loss', val_loss, n_iter)\n",
    "                writer.add_scalar('data/val_acc', val_acc, n_iter)\n",
    "                n_iter += 1\n",
    "                \n",
    "def apply(val, model_ff, batch_size=33, criterion = nn.BCEWithLogitsLoss()):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    its = 0\n",
    "    model_ff.train(False)\n",
    "    for b in np.arange(0, val.y.shape[0], batch_size):\n",
    "        its += 1\n",
    "        X_batch = torch.FloatTensor(val.question[b:b+batch_size])\n",
    "        y_batch = torch.FloatTensor(val.y[b:b+batch_size])\n",
    "        a_batch = torch.FloatTensor(val.answer[b:b+batch_size])\n",
    "        with torch.no_grad():\n",
    "            y_pred_logits = model_ff(X_batch, a_batch)\n",
    "        loss = criterion(y_pred_logits, y_batch)\n",
    "#                     print(X_batch.shape, y_batch.shape, y_pred_logits.shape)\n",
    "        s_pred = torch.argmax(y_pred_logits.reshape(-1, 3), dim=1)\n",
    "        s_true = torch.argmax(y_batch.reshape(-1, 3), dim=1)\n",
    "        acc = torch.mean((s_pred == s_true).type(torch.FloatTensor))\n",
    "        val_loss += loss.item()\n",
    "        val_acc += acc.item()\n",
    "    val_loss /= its\n",
    "    val_acc /= its\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 768  704  726  215 1158  813  436 1611  148 1423]\n"
     ]
    }
   ],
   "source": [
    "ids = np.arange(len(ext_bundle))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(ids)\n",
    "print(ids[:10])\n",
    "N = int(len(ext_bundle) * 0.2)\n",
    "val_ids = ids[:N]\n",
    "train_ids = ids[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(bundle: dict)->np.array:\n",
    "    e = bundle['embed'][0]\n",
    "    return np.concatenate([np.max(e, axis=0), e[-1]],axis=0).reshape(-1)  # last + max pooling\n",
    "\n",
    "def make_dataset(bundle, targets, ids) -> Dataset:\n",
    "    assert len(bundle) == len(targets)\n",
    "    Q,A,Y = [],[],[]\n",
    "    for idx in ids:\n",
    "        q = get_embed(bundle[idx]['q'])\n",
    "        a = []\n",
    "        for j in range(3):\n",
    "            a = get_embed(bundle[idx]['a{}'.format(j + 1)])\n",
    "            Q.append(q)\n",
    "            A.append(a)\n",
    "            Y.append([targets[idx] == j])\n",
    "    Q = np.array(Q, dtype=np.float32)\n",
    "    A = np.array(A, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "    return Dataset(Q, A, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainD = make_dataset(ext_bundle, prepared_en_ru_targets, train_ids)\n",
    "valD = make_dataset(ext_bundle, prepared_en_ru_targets, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModelV1(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H):\n",
    "        super().__init__()\n",
    "        self.model_ff =  nn.Sequential(\n",
    "            nn.BatchNorm1d(INPUT_F),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, q, a):\n",
    "        x = torch.cat([q, a], dim=1)\n",
    "        return self.model_ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 40 batch: 0.68664\n",
      "val loss in 1 epoch: 0.68680\n",
      "val acc in 1 epoch: 0.30724\n",
      "train loss in 1 epoch in 80 batch: 0.68539\n",
      "val loss in 1 epoch: 0.68151\n",
      "val acc in 1 epoch: 0.30471\n",
      "train loss in 1 epoch in 120 batch: 0.66841\n",
      "val loss in 1 epoch: 0.67489\n",
      "val acc in 1 epoch: 0.31145\n",
      "train loss in 2 epoch in 40 batch: 0.66671\n",
      "val loss in 2 epoch: 0.66420\n",
      "val acc in 2 epoch: 0.33670\n",
      "train loss in 2 epoch in 80 batch: 0.61747\n",
      "val loss in 2 epoch: 0.65773\n",
      "val acc in 2 epoch: 0.34428\n",
      "train loss in 2 epoch in 120 batch: 0.64184\n",
      "val loss in 2 epoch: 0.65246\n",
      "val acc in 2 epoch: 0.32155\n",
      "train loss in 3 epoch in 40 batch: 0.62614\n",
      "val loss in 3 epoch: 0.64878\n",
      "val acc in 3 epoch: 0.32407\n",
      "train loss in 3 epoch in 80 batch: 0.65755\n",
      "val loss in 3 epoch: 0.64639\n",
      "val acc in 3 epoch: 0.31902\n",
      "train loss in 3 epoch in 120 batch: 0.65796\n",
      "val loss in 3 epoch: 0.64403\n",
      "val acc in 3 epoch: 0.32660\n",
      "train loss in 4 epoch in 40 batch: 0.61400\n",
      "val loss in 4 epoch: 0.64231\n",
      "val acc in 4 epoch: 0.32155\n",
      "train loss in 4 epoch in 80 batch: 0.62730\n",
      "val loss in 4 epoch: 0.64116\n",
      "val acc in 4 epoch: 0.32660\n",
      "train loss in 4 epoch in 120 batch: 0.61042\n",
      "val loss in 4 epoch: 0.64048\n",
      "val acc in 4 epoch: 0.32660\n",
      "train loss in 5 epoch in 40 batch: 0.69782\n",
      "val loss in 5 epoch: 0.63923\n",
      "val acc in 5 epoch: 0.32660\n",
      "train loss in 5 epoch in 80 batch: 0.65509\n",
      "val loss in 5 epoch: 0.63860\n",
      "val acc in 5 epoch: 0.32912\n",
      "train loss in 5 epoch in 120 batch: 0.58581\n",
      "val loss in 5 epoch: 0.63795\n",
      "val acc in 5 epoch: 0.32912\n",
      "train loss in 6 epoch in 40 batch: 0.64903\n",
      "val loss in 6 epoch: 0.63709\n",
      "val acc in 6 epoch: 0.33418\n",
      "train loss in 6 epoch in 80 batch: 0.66189\n",
      "val loss in 6 epoch: 0.63643\n",
      "val acc in 6 epoch: 0.33923\n",
      "train loss in 6 epoch in 120 batch: 0.65850\n",
      "val loss in 6 epoch: 0.63585\n",
      "val acc in 6 epoch: 0.33418\n",
      "train loss in 7 epoch in 40 batch: 0.72520\n",
      "val loss in 7 epoch: 0.63487\n",
      "val acc in 7 epoch: 0.34680\n",
      "train loss in 7 epoch in 80 batch: 0.64243\n",
      "val loss in 7 epoch: 0.63425\n",
      "val acc in 7 epoch: 0.34175\n",
      "train loss in 7 epoch in 120 batch: 0.65981\n",
      "val loss in 7 epoch: 0.63402\n",
      "val acc in 7 epoch: 0.34933\n",
      "train loss in 8 epoch in 40 batch: 0.63591\n",
      "val loss in 8 epoch: 0.63370\n",
      "val acc in 8 epoch: 0.35185\n",
      "train loss in 8 epoch in 80 batch: 0.62006\n",
      "val loss in 8 epoch: 0.63297\n",
      "val acc in 8 epoch: 0.35185\n",
      "train loss in 8 epoch in 120 batch: 0.64240\n",
      "val loss in 8 epoch: 0.63261\n",
      "val acc in 8 epoch: 0.35690\n",
      "train loss in 9 epoch in 40 batch: 0.67599\n",
      "val loss in 9 epoch: 0.63153\n",
      "val acc in 9 epoch: 0.36869\n",
      "train loss in 9 epoch in 80 batch: 0.59944\n",
      "val loss in 9 epoch: 0.63107\n",
      "val acc in 9 epoch: 0.36616\n",
      "train loss in 9 epoch in 120 batch: 0.55050\n",
      "val loss in 9 epoch: 0.63076\n",
      "val acc in 9 epoch: 0.36111\n",
      "train loss in 10 epoch in 40 batch: 0.57270\n",
      "val loss in 10 epoch: 0.63046\n",
      "val acc in 10 epoch: 0.35859\n",
      "train loss in 10 epoch in 80 batch: 0.59327\n",
      "val loss in 10 epoch: 0.63006\n",
      "val acc in 10 epoch: 0.36111\n",
      "train loss in 10 epoch in 120 batch: 0.63802\n",
      "val loss in 10 epoch: 0.62982\n",
      "val acc in 10 epoch: 0.36111\n",
      "train loss in 11 epoch in 40 batch: 0.55303\n",
      "val loss in 11 epoch: 0.62942\n",
      "val acc in 11 epoch: 0.36364\n",
      "train loss in 11 epoch in 80 batch: 0.59105\n",
      "val loss in 11 epoch: 0.62917\n",
      "val acc in 11 epoch: 0.36869\n",
      "train loss in 11 epoch in 120 batch: 0.56758\n",
      "val loss in 11 epoch: 0.62905\n",
      "val acc in 11 epoch: 0.36111\n",
      "train loss in 12 epoch in 40 batch: 0.56360\n",
      "val loss in 12 epoch: 0.62836\n",
      "val acc in 12 epoch: 0.36616\n",
      "train loss in 12 epoch in 80 batch: 0.69548\n",
      "val loss in 12 epoch: 0.62805\n",
      "val acc in 12 epoch: 0.37626\n",
      "train loss in 12 epoch in 120 batch: 0.66908\n",
      "val loss in 12 epoch: 0.62776\n",
      "val acc in 12 epoch: 0.36616\n",
      "train loss in 13 epoch in 40 batch: 0.53006\n",
      "val loss in 13 epoch: 0.62742\n",
      "val acc in 13 epoch: 0.36869\n",
      "train loss in 13 epoch in 80 batch: 0.56062\n",
      "val loss in 13 epoch: 0.62715\n",
      "val acc in 13 epoch: 0.37374\n",
      "train loss in 13 epoch in 120 batch: 0.60548\n",
      "val loss in 13 epoch: 0.62713\n",
      "val acc in 13 epoch: 0.37879\n",
      "train loss in 14 epoch in 40 batch: 0.60755\n",
      "val loss in 14 epoch: 0.62700\n",
      "val acc in 14 epoch: 0.37121\n",
      "train loss in 14 epoch in 80 batch: 0.54182\n",
      "val loss in 14 epoch: 0.62689\n",
      "val acc in 14 epoch: 0.36616\n",
      "train loss in 14 epoch in 120 batch: 0.64823\n",
      "val loss in 14 epoch: 0.62673\n",
      "val acc in 14 epoch: 0.36869\n",
      "train loss in 15 epoch in 40 batch: 0.60740\n",
      "val loss in 15 epoch: 0.62630\n",
      "val acc in 15 epoch: 0.37121\n",
      "train loss in 15 epoch in 80 batch: 0.53750\n",
      "val loss in 15 epoch: 0.62602\n",
      "val acc in 15 epoch: 0.37121\n",
      "train loss in 15 epoch in 120 batch: 0.55613\n",
      "val loss in 15 epoch: 0.62573\n",
      "val acc in 15 epoch: 0.37374\n",
      "train loss in 16 epoch in 40 batch: 0.73676\n",
      "val loss in 16 epoch: 0.62580\n",
      "val acc in 16 epoch: 0.37879\n",
      "train loss in 16 epoch in 80 batch: 0.64327\n",
      "val loss in 16 epoch: 0.62589\n",
      "val acc in 16 epoch: 0.38636\n",
      "train loss in 16 epoch in 120 batch: 0.63086\n",
      "val loss in 16 epoch: 0.62523\n",
      "val acc in 16 epoch: 0.37879\n",
      "train loss in 17 epoch in 40 batch: 0.60108\n",
      "val loss in 17 epoch: 0.62525\n",
      "val acc in 17 epoch: 0.36616\n",
      "train loss in 17 epoch in 80 batch: 0.58697\n",
      "val loss in 17 epoch: 0.62486\n",
      "val acc in 17 epoch: 0.37121\n",
      "train loss in 17 epoch in 120 batch: 0.57055\n",
      "val loss in 17 epoch: 0.62487\n",
      "val acc in 17 epoch: 0.37121\n",
      "train loss in 18 epoch in 40 batch: 0.69889\n",
      "val loss in 18 epoch: 0.62519\n",
      "val acc in 18 epoch: 0.37121\n",
      "train loss in 18 epoch in 80 batch: 0.62258\n",
      "val loss in 18 epoch: 0.62448\n",
      "val acc in 18 epoch: 0.37374\n",
      "train loss in 18 epoch in 120 batch: 0.57972\n",
      "val loss in 18 epoch: 0.62455\n",
      "val acc in 18 epoch: 0.36869\n",
      "train loss in 19 epoch in 40 batch: 0.56380\n",
      "val loss in 19 epoch: 0.62451\n",
      "val acc in 19 epoch: 0.37374\n",
      "train loss in 19 epoch in 80 batch: 0.67891\n",
      "val loss in 19 epoch: 0.62466\n",
      "val acc in 19 epoch: 0.37879\n",
      "train loss in 19 epoch in 120 batch: 0.53218\n",
      "val loss in 19 epoch: 0.62442\n",
      "val acc in 19 epoch: 0.37374\n",
      "train loss in 20 epoch in 40 batch: 0.50582\n",
      "val loss in 20 epoch: 0.62415\n",
      "val acc in 20 epoch: 0.37121\n",
      "train loss in 20 epoch in 80 batch: 0.67643\n",
      "val loss in 20 epoch: 0.62402\n",
      "val acc in 20 epoch: 0.38131\n",
      "train loss in 20 epoch in 120 batch: 0.51072\n",
      "val loss in 20 epoch: 0.62397\n",
      "val acc in 20 epoch: 0.38384\n",
      "train loss in 21 epoch in 40 batch: 0.49974\n",
      "val loss in 21 epoch: 0.62403\n",
      "val acc in 21 epoch: 0.37879\n",
      "train loss in 21 epoch in 80 batch: 0.63784\n",
      "val loss in 21 epoch: 0.62416\n",
      "val acc in 21 epoch: 0.38384\n",
      "train loss in 21 epoch in 120 batch: 0.57498\n",
      "val loss in 21 epoch: 0.62442\n",
      "val acc in 21 epoch: 0.37374\n",
      "train loss in 22 epoch in 40 batch: 0.56006\n",
      "val loss in 22 epoch: 0.62380\n",
      "val acc in 22 epoch: 0.38384\n",
      "train loss in 22 epoch in 80 batch: 0.54561\n",
      "val loss in 22 epoch: 0.62380\n",
      "val acc in 22 epoch: 0.37879\n",
      "train loss in 22 epoch in 120 batch: 0.61177\n",
      "val loss in 22 epoch: 0.62405\n",
      "val acc in 22 epoch: 0.38131\n",
      "train loss in 23 epoch in 40 batch: 0.59603\n",
      "val loss in 23 epoch: 0.62402\n",
      "val acc in 23 epoch: 0.36616\n",
      "train loss in 23 epoch in 80 batch: 0.71379\n",
      "val loss in 23 epoch: 0.62433\n",
      "val acc in 23 epoch: 0.37626\n",
      "train loss in 23 epoch in 120 batch: 0.58000\n",
      "val loss in 23 epoch: 0.62398\n",
      "val acc in 23 epoch: 0.38384\n",
      "train loss in 24 epoch in 40 batch: 0.60294\n",
      "val loss in 24 epoch: 0.62391\n",
      "val acc in 24 epoch: 0.37374\n",
      "train loss in 24 epoch in 80 batch: 0.57806\n",
      "val loss in 24 epoch: 0.62500\n",
      "val acc in 24 epoch: 0.36616\n",
      "train loss in 24 epoch in 120 batch: 0.49376\n",
      "val loss in 24 epoch: 0.62412\n",
      "val acc in 24 epoch: 0.38131\n",
      "train loss in 25 epoch in 40 batch: 0.50410\n",
      "val loss in 25 epoch: 0.62328\n",
      "val acc in 25 epoch: 0.38636\n",
      "train loss in 25 epoch in 80 batch: 0.58699\n",
      "val loss in 25 epoch: 0.62338\n",
      "val acc in 25 epoch: 0.37374\n",
      "train loss in 25 epoch in 120 batch: 0.59313\n",
      "val loss in 25 epoch: 0.62404\n",
      "val acc in 25 epoch: 0.37374\n",
      "train loss in 26 epoch in 40 batch: 0.48606\n",
      "val loss in 26 epoch: 0.62380\n",
      "val acc in 26 epoch: 0.36869\n",
      "train loss in 26 epoch in 80 batch: 0.63937\n",
      "val loss in 26 epoch: 0.62406\n",
      "val acc in 26 epoch: 0.38636\n",
      "train loss in 26 epoch in 120 batch: 0.53886\n",
      "val loss in 26 epoch: 0.62412\n",
      "val acc in 26 epoch: 0.37626\n",
      "train loss in 27 epoch in 40 batch: 0.58922\n",
      "val loss in 27 epoch: 0.62391\n",
      "val acc in 27 epoch: 0.37626\n",
      "train loss in 27 epoch in 80 batch: 0.46830\n",
      "val loss in 27 epoch: 0.62399\n",
      "val acc in 27 epoch: 0.37121\n",
      "train loss in 27 epoch in 120 batch: 0.65197\n",
      "val loss in 27 epoch: 0.62377\n",
      "val acc in 27 epoch: 0.38384\n",
      "train loss in 28 epoch in 40 batch: 0.54792\n",
      "val loss in 28 epoch: 0.62349\n",
      "val acc in 28 epoch: 0.37121\n",
      "train loss in 28 epoch in 80 batch: 0.55567\n",
      "val loss in 28 epoch: 0.62325\n",
      "val acc in 28 epoch: 0.37879\n",
      "train loss in 28 epoch in 120 batch: 0.57425\n",
      "val loss in 28 epoch: 0.62361\n",
      "val acc in 28 epoch: 0.37121\n",
      "train loss in 29 epoch in 40 batch: 0.52336\n",
      "val loss in 29 epoch: 0.62432\n",
      "val acc in 29 epoch: 0.38131\n",
      "train loss in 29 epoch in 80 batch: 0.62491\n",
      "val loss in 29 epoch: 0.62430\n",
      "val acc in 29 epoch: 0.38636\n",
      "train loss in 29 epoch in 120 batch: 0.59347\n",
      "val loss in 29 epoch: 0.62433\n",
      "val acc in 29 epoch: 0.38889\n",
      "train loss in 30 epoch in 40 batch: 0.53491\n",
      "val loss in 30 epoch: 0.62434\n",
      "val acc in 30 epoch: 0.37879\n",
      "train loss in 30 epoch in 80 batch: 0.51090\n",
      "val loss in 30 epoch: 0.62444\n",
      "val acc in 30 epoch: 0.38384\n",
      "train loss in 30 epoch in 120 batch: 0.54375\n",
      "val loss in 30 epoch: 0.62461\n",
      "val acc in 30 epoch: 0.38131\n"
     ]
    }
   ],
   "source": [
    "INPUT_F = 1536 * 4\n",
    "H = 128\n",
    "DROP_P = 0.1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model = NNModelV1(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(trainD, valD, model, freq=40, batch_size=33,lr=1e-5, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_trainD = make_dataset(train_bundle, train_raw_targets, np.arange(len(train_raw_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_extD = make_dataset(ext_bundle, prepared_en_ru_targets, np.arange(len(prepared_en_ru_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6405012151679477, 0.36191647149421075)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(original_trainD, model) # drop 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'models/sber_gpt3_hidden_to_target_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_F = 1536 * 4\n",
    "H = 128\n",
    "DROP_P = 0.1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model = NNModelV1(INPUT_F, DROP_P, H)\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/sber_gpt3_hidden_to_target_v1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_and_save_predictions(data, file):\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.FloatTensor(data.question), torch.FloatTensor(data.answer))\n",
    "    logits_prepared = logits.reshape(-1).cpu().detach().numpy()\n",
    "    print(logits_prepared.shape)\n",
    "    np.save(file, logits_prepared)\n",
    "    return logits_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12183,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.86392146, -0.87211543, -0.79791766, ..., -0.45517492,\n",
       "       -0.42451477, -0.52346647], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_and_save_predictions(original_trainD, \"data/original_train_predictions_by_model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5823,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.8281525 , -0.8319542 , -0.5934919 , ..., -0.5707167 ,\n",
       "       -0.75683475, -0.72892743], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_and_save_predictions(full_extD, \"data/ext_predictions_by_model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ext_4K_dataset = np.load(\"data/en_ru_4k_dataset.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0,\n",
       "        'Компания Loake Brothers, основанная в 1880 году, известна своим высоким качеством.',\n",
       "        'The Lion King', 'Баптистская церковь Уэстборо', 'Обувь', 2],\n",
       "       [1, '\"Кто играет главную роль в фильме\" Бэтмен \"?',\n",
       "        'Уиттингтон, Ричард', 'Мишель Баффер',\n",
       "        'Ошибки, допущенные в ходе выборов, могут привести к тому, что многие избиратели не смогут проголосовать за кандидата в президенты США от Республиканской партии Дональда Трампа, который, по их мнению, является наиболее вероятным кандидатом от Республиканской партии на предстоящих президентских выборах в 2016 году.',\n",
       "        1],\n",
       "       [2, 'Под каким названием проходят съемки фильма \"Лед и пламя\"?',\n",
       "        'Игра престолов', 'Жан Симмонс', 'Мэри Квант', 0],\n",
       "       ...,\n",
       "       [4093,\n",
       "        'Что представляет собой европейская система оценки качества образования?',\n",
       "        'Мидлмарш', 'Дидо', 'Альбатрос', 2],\n",
       "       [4094, 'В 1999 году Том Хэнкс', 'История игрушек 2',\n",
       "        'Бетьман, Джон', 'Бриннер, Юл', 0],\n",
       "       [4095,\n",
       "        'В то же время, по его словам, в настоящее время, несмотря на то, что большинство стран, в том числе и Россия, являются членами НАТО, они не готовы к тому, чтобы в будущем стать членами этой организации, а также к тому, чтобы стать ее членами, в том числе и из-за того, что у них нет достаточного опыта, необходимого для того, чтобы справиться с этой проблемой.',\n",
       "        'Гонконг', 'Изображения людей с палками', 'Миа Фэрроу', 0]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ext_4K_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 2, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ext_4K_targets = tr_ext_4K_dataset[:, -1]\n",
    "tr_ext_4K_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ext_4K_bundle = np.load(\"data/en_ru_4k_bundle.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ext_4KD = make_dataset(tr_ext_4K_bundle, tr_ext_4K_targets, np.arange(len(tr_ext_4K_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6538657828885812, 0.38148916358922186)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(tr_ext_4KD, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.47767383, -0.8918863 , -0.74630445, ..., -0.7757869 ,\n",
       "       -1.3973103 , -1.2401094 ], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_and_save_predictions(tr_ext_4KD, \"data/tr_ext_4K_predictions_by_model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubq_qat_pairs = np.load(\"data/rubq_qat_pairs.npy\", allow_pickle=True)\n",
    "rubq_bundle = [x[-1] for x in rubq_qat_pairs]\n",
    "rubq_targets = [x[-2] for x in rubq_qat_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubqD = make_dataset(rubq_bundle, rubq_targets, np.arange(len(rubq_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6379615324285796, 0.3739756146129572)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(rubqD, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6990,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.7742601 , -0.83825415, -0.83260673, ..., -0.46999604,\n",
       "       -0.5549119 , -0.5293012 ], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_and_save_predictions(rubqD, \"data/rubq_predictions_by_model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
